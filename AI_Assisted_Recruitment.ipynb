{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HxO60AMK_dx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiaTSuo_AHHe",
    "outputId": "ab0bea20-2ff5-4e5a-a4f4-1bd13f407bd5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IfsMtzY2dsB"
   },
   "source": [
    "All of our Google Drive files look the same, so that's where we are pulling our data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb4RXPXfA4Ps",
    "outputId": "1e3a00a5-60f3-40d6-e43a-d7c6f541284a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = \"/content/drive/MyDrive/cadence 1a/data\"\n",
    "print(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnQKUhFH2lld"
   },
   "source": [
    "We read the resume dataset and do some preliminary analysis to see howw it looks. We look at the null values, the number of columns and rows, as well as the sum of null values. We proceed to do the same for the job posts dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UiC5YeKgBUKp",
    "outputId": "b5d21c31-9a39-4f5c-d980-75f01b43dd9a"
   },
   "outputs": [],
   "source": [
    "resume_df = pd.read_csv(f\"{data_path}/Resume.csv\")\n",
    "resume_df.shape\n",
    "resume_df.head()\n",
    "# resume_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "igUkm9NdRWQA",
    "outputId": "d2470e6a-5ff5-476e-b8b2-90beb0afccd4"
   },
   "outputs": [],
   "source": [
    "job_posts_df = pd.read_csv(f\"{data_path}/data job posts.csv\")\n",
    "job_posts_df.shape\n",
    "job_posts_df.head()\n",
    "# job_posts_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcHnyhf9221a"
   },
   "source": [
    "Columns that have missing values in the job posts dataset are being turned into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2E757sJxRuaV",
    "outputId": "2676decd-72f2-4d07-93fe-40faebe6ce75"
   },
   "outputs": [],
   "source": [
    "condition = job_posts_df.isnull().sum() != 0\n",
    "job_posts_df.isnull().sum()[condition].index\n",
    "columnlist = list(job_posts_df.isnull().sum()[condition].index)\n",
    "columnlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhBidoMS3NFU"
   },
   "source": [
    "There are no numerical values in this data set; everything is stored in string/object format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "URFmCMdoXRBC",
    "outputId": "412ac215-58d7-4e99-8c0f-7a74a20afdee"
   },
   "outputs": [],
   "source": [
    "job_posts_df[columnlist].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gVcVpRye9Dp"
   },
   "source": [
    "## Addressing Null values in job_posts_df\n",
    "Since there are many columns in job_posts_df with null values, we can use reasoning to drop some of the rows or columns. Some columns can be cut if they have too many null values or if they are not really relevant to the problem.  For example, the column \"AnnouncementCode\" has 17793 null values and the unique non-null values are a random string of letters. It was most likely used to identify the job posting on its original website. As well, \"Opening Date\" and \"Deadline\" may not have many null values, but these values might not be very useful to determine whether or not a candidate would be a good fit for a job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "QACcPPTBaAiY",
    "outputId": "bd03e616-205d-49c5-e124-3ab62f3566bf"
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['AnnouncementCode', 'Term', 'Eligibility', 'Audience', 'StartDate', 'Duration', 'OpeningDate', 'Deadline', 'Notes', 'Attach']\n",
    "job_posts_df = job_posts_df.drop(columns=columns_to_drop)\n",
    "job_posts_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2ZAjOMuiirN"
   },
   "source": [
    "There are still null values, but the columns are too contextually important to the ML problem. We can drop the examples that have null values in these columns since they most likely do not have the information we need to train the model accurately. After we drop these rows, our job_posts_df dataset no longer has any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83OGw9jigbBQ",
    "outputId": "c4f0afd9-0b87-4abc-aac7-c9b0fffd4ff2"
   },
   "outputs": [],
   "source": [
    "columns_to_check = ['Title', 'JobDescription', 'JobRequirment', 'Company', 'Location', 'RequiredQual', 'Salary', 'AboutC']\n",
    "\n",
    "job_posts_df = job_posts_df.dropna(subset=columns_to_check)\n",
    "\n",
    "job_posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "jTtLm_A4hk8J",
    "outputId": "2bae69b1-4307-4e43-c354-8e4cd06512b7"
   },
   "outputs": [],
   "source": [
    "job_posts_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tB5bFRjO3sL6"
   },
   "source": [
    "Removing duplicates from both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjVSiapQqOyl",
    "outputId": "70219f57-1e82-4eb4-a63a-f38606887ba4"
   },
   "outputs": [],
   "source": [
    "job_posts_df = job_posts_df.drop_duplicates()\n",
    "print(job_posts_df.duplicated().sum())\n",
    "resume_df = resume_df.drop_duplicates()\n",
    "print(resume_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjgA3eMW3xbm"
   },
   "source": [
    "Finding columns with HTML tags. No column in job post has any HTML tags we need to remove. Only resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FOg5qmAsLF0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def has_html(text):\n",
    "    if isinstance(text, str):\n",
    "        return bool(re.search(r'<.*?>', text))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZduWLCTspLd",
    "outputId": "25e550fb-f374-407b-b153-a4b98b2247d4"
   },
   "outputs": [],
   "source": [
    "columns_with_html = [col for col in job_posts_df.columns if job_posts_df[col].apply(has_html).any()]\n",
    "print(columns_with_html)\n",
    "\n",
    "columns_with_html = [col for col in resume_df.columns if resume_df[col].apply(has_html).any()]\n",
    "print(columns_with_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmPWzI_frHOD"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_html(text):\n",
    "  return re.sub('<[^<]+?>', '', text)\n",
    "\n",
    "resume_df['Resume_html'] = resume_df['Resume_str'].apply(clean_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byjNEVv43-NG"
   },
   "source": [
    "Resume_html and Resume_str are the same column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhN8VCwlry0E"
   },
   "outputs": [],
   "source": [
    "resume_df[['Resume_html', 'Resume_str']].head()\n",
    "resume_df.drop(columns=['Resume_html'], inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgMICFDzEng_"
   },
   "source": [
    "Creates a corpus for all job post related details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5V8QYCpECSN"
   },
   "outputs": [],
   "source": [
    "job_posts_df[\"job_text\"] = (\n",
    "    \"Description: \" + job_posts_df[\"JobDescription\"].fillna('') + \" \"\n",
    "    \"Requirements: \" + job_posts_df[\"JobRequirment\"].fillna('') + \" \"\n",
    "    \"Qualifications: \" + job_posts_df[\"RequiredQual\"].fillna('') + \" \"\n",
    "    \"About Company: \" + job_posts_df[\"AboutC\"].fillna('')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scu-RU6EGFQa"
   },
   "source": [
    "# Checkpoint #2 - Text Normalization\n",
    "Apply tokenization, lowercasing, stopword removal, and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OzDqZxH8BFd"
   },
   "source": [
    "Import NLP and text-processing tools:\n",
    "- nltk for natural language processing utilities (downloads WordNet for lemmatization)\n",
    "- TfidfVectorizer and ENGLISH_STOP_WORDS from sklearn to convert text into numerical features and remove common stop words\n",
    "- WordNetLemmatizer to reduce words to their base (dictionary) form\n",
    "- word_tokenize: splits sentences into individual words\n",
    "- stopwords: provides common words (like \"the\", \"and\") to remove from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06Rgh2QGQTL",
    "outputId": "ba3e4075-b101-450c-a410-907f4694d2cd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTKrkD358l2T"
   },
   "source": [
    "Make sure NLTK resources are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmMAYi1c8oLR",
    "outputId": "e596a60c-0689-42b8-83d6-0c0d15c44119"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFbtCv-s9jwA"
   },
   "source": [
    "Initialize the WordNet lemmatizer and define a set of English stopwords for text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_uCAasx9fIL"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0--N5H6fB56Z"
   },
   "source": [
    "\n",
    " TEXT PREPROCESSING PIPELINE FOR RESUMES AND JOB POSTS\n",
    "\n",
    " Purpose:\n",
    "   This section standardizes and cleans all text data to prepare it\n",
    "   for accurate keyword and semantic matching in the scoring system.\n",
    "\n",
    " Description:\n",
    "   The preprocess_text() function normalizes text by performing:\n",
    "     1. Lowercasing – ensures consistent word comparisons.\n",
    "     2. Tokenization – splits text into individual words.\n",
    "     3. Stopword removal – removes common filler words like \"the\", \"and\", \"is\".\n",
    "     4. Filtering – keeps only alphabetic tokens (drops numbers/punctuation).\n",
    "     5. Lemmatization – reduces words to their base form\n",
    "        (e.g., “running” → “run”, “analyses” → “analysis”).\n",
    "     6. Reconstruction – joins cleaned tokens back into a single string.\n",
    "\n",
    "   This preprocessing is applied to:\n",
    "     • resume_df[\"Resume_str\"]  → creates resume_df[\"Resume_clean\"]\n",
    "     • job_posts_df[\"job_text\"] → creates job_posts_df[\"job_text_clean\"]\n",
    "\n",
    " Outcome:\n",
    "   Produces standardized, lemmatized, and stopword-free text columns\n",
    "   for both resumes and job postings, enabling fair and consistent#   matching in later scoring functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7wo8FRRB4ZQ"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Lowercase, tokenize, remove stopwords, and lemmatize.\"\"\"\n",
    "    tokens = word_tokenize(str(text).lower())\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "resume_df[\"Resume_clean\"] = resume_df[\"Resume_str\"].apply(preprocess_text)\n",
    "job_posts_df['job_text_clean'] = job_posts_df['job_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jM8ur0s87IP"
   },
   "source": [
    "Tokenizes input text, converts it to lowercase, removes punctuation and stopwords, and lemmatizes each word to its base form for cleaner, standardized text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3i_CvUfHYwq"
   },
   "outputs": [],
   "source": [
    "# def lemmatize_and_tokenize(text):\n",
    "#     tokens = word_tokenize(str(text).lower())   # lowercase + tokenize\n",
    "#     tokens = [lemmatizer.lemmatize(t) for t in tokens\n",
    "#               if t.isalpha() and t not in stop_words]  # keep only words, no stopwords\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXl9JGmA90y3"
   },
   "source": [
    "Create a TF-IDF vectorizer that uses the custom lemmatize_and_tokenize function to transform resume text into numerical feature vectors.\n",
    "Apply it only to the 'Resume_str' column of the DataFrame, which contains the text data.\n",
    "The resulting TF-IDF matrix shows how many resumes (rows) and unique terms (columns) were processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4NlkbgS9usP"
   },
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(tokenizer=lemmatize_and_tokenize)\n",
    "\n",
    "# # Important: pass only the resume text column\n",
    "# tfidf_matrix_resumes = vectorizer.fit_transform(resume_df[\"Resume_str\"])\n",
    "\n",
    "# print(\"Shape of tfidf_matrix_resumes:\", tfidf_matrix_resumes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qckqJd4NIEoO"
   },
   "outputs": [],
   "source": [
    "# #Fit/transform our the resumes\n",
    "\n",
    "# # This does everything in one call:\n",
    "# # 1. Normalizes (lowercase, stopwords, lemmatization)\n",
    "# # 2. Creates the vocabulary\n",
    "# # 3. Calculates TF-IDF vectors\n",
    "# tfidf_matrix_resumes = vectorizer.fit_transform(resume_df)\n",
    "\n",
    "# print(\"Shape of tfidf_matrix_resumes:\", tfidf_matrix_resumes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lmqu4BPrPssA"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Pick a few resumes to inspect\n",
    "# sample_indices = [0, 1, 2]  # change these to any resume indices you want\n",
    "# pd.set_option(\"display.max_colwidth\", None)\n",
    "# for idx in sample_indices:\n",
    "#     original = resume_df[\"Resume_str\"].iloc[idx][:500]  # first 500 chars only for readability\n",
    "#     normalized = lemmatize_and_tokenize(resume_df[\"Resume_str\"].iloc[idx])\n",
    "\n",
    "#     print(f\"=== Resume {idx} ===\")\n",
    "#     print(\"Original snippet:\")\n",
    "#     print(original, \"\\n\")\n",
    "#     print(\"Normalized tokens:\")\n",
    "#     print(normalized[:30], \"...\")  # show first 30 tokens\n",
    "#     print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DQNQwh7Zuu_"
   },
   "outputs": [],
   "source": [
    "# # Show the full text in each cell\n",
    "# pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# # Now when you print, you’ll see everything\n",
    "# resume_df[\"Resume_str\"].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcK_wcduThwU"
   },
   "outputs": [],
   "source": [
    "# See the structure\n",
    "# print(job_posts_df.info())\n",
    "\n",
    "# See a few sample job posts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARpu4bP9T4xR"
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_colwidth\", None)\n",
    "# job_posts_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iZWtf3XTGxk"
   },
   "source": [
    "# Checkpoint #3 - Data Annotation\n",
    "\n",
    "define weights for: skills, experience, education, semantic similarity, domain\n",
    "creates labels (good match/top 25% = 1, bad match/bottom 25% = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22PQECrK_4Bo"
   },
   "source": [
    "\n",
    "# SCORING CONFIGURATION AND KEYWORD DEFINITIONS\n",
    "\n",
    " This section defines:\n",
    " 1. Feature weights — how much each category contributes to the final composite score.\n",
    " 2. Domain-specific keywords — used to detect the job domain (e.g., HR, finance, IT).\n",
    " 3. Common skill keywords — used to evaluate skill overlap between job and resume.\n",
    "4. Education levels — mapped to numeric values for scoring academic background.\n",
    " 5. Experience indicators — keywords representing seniority or management experience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7D65CJJAxuR"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ckQe9XcTrU_"
   },
   "outputs": [],
   "source": [
    "#define weights\n",
    "weights = {\n",
    "    'skills': 0.35,\n",
    "    'experience': 0.20,\n",
    "    'education': 0.15,\n",
    "    'semantic': 0.15,\n",
    "    'domain': 0.15\n",
    "}\n",
    "\n",
    "\n",
    "domain_keywords = {\n",
    "    'hr': ['human resources', 'hr', 'recruitment', 'recruiting', 'hiring', 'payroll', 'benefits', 'employee relations', 'compensation', 'performance management'],\n",
    "    'finance': ['finance', 'financial', 'accounting', 'budget', 'audit', 'tax', 'bookkeeping', 'financial analysis'],\n",
    "    'it': ['programming', 'software', 'development', 'python', 'java', 'sql', 'database', 'web development', 'network', 'system administration'],\n",
    "    'sales': ['sales', 'business development', 'account management', 'revenue', 'crm'],\n",
    "    'administration': ['administrative', 'secretary', 'assistant', 'coordination', 'office'],\n",
    "    'research': ['research', 'analyst', 'analysis', 'data analysis', 'methodology']\n",
    "}\n",
    "\n",
    "skills = ['excel', 'word', 'powerpoint', 'sql', 'python', 'project management', 'data analysis', 'ms office', 'microsoft office']\n",
    "\n",
    "education_levels = {'phd': 4, 'master': 3, 'bachelor': 2, 'associate': 1, 'diploma': 0.5}\n",
    "\n",
    "experience_words = ['manager', 'director', 'senior', 'lead', 'specialist', 'analyst']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eA1zSCsAfOs"
   },
   "source": [
    "   Computes a basic semantic similarity between a resume and job posting.\n",
    "\n",
    "    Steps:\n",
    "    1. Converts both texts to lowercase.\n",
    "    2. Splits them into sets of unique words.\n",
    "    3. Finds the overlap (intersection) between the two word sets.\n",
    "    4. Returns the proportion of job words also present in the resume.\n",
    "\n",
    "    Note:\n",
    "        This is a simple lexical overlap metric, not a deep semantic one.\n",
    "        It can later be replaced with embedding-based cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        resume_text (str): Resume text.\n",
    "        job_text (str): Job posting text.\n",
    "\n",
    "    Returns:\n",
    "        float: Overlap ratio between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0tKyuA_UQZn"
   },
   "outputs": [],
   "source": [
    "def calculate_semantic_score(resume_text, job_text):\n",
    "    resume_words = set(resume_text.lower().split())\n",
    "    job_words = set(job_text.lower().split())\n",
    "    if not job_words:\n",
    "        return 0\n",
    "    overlap = len(resume_words.intersection(job_words))\n",
    "    return overlap / len(job_words) if overlap > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMZB0lqbAkvh"
   },
   "source": [
    "    Calculates how well a resume matches the skills required by a job posting.\n",
    "\n",
    "    Steps:\n",
    "    1. Converts both resume and job descriptions to lowercase.\n",
    "    2. Extracts all skill keywords that appear in the job text.\n",
    "    3. Checks which of those required skills also appear in the resume.\n",
    "    4. Returns the ratio of matched skills to total job-required skills.\n",
    "\n",
    "    Args:\n",
    "        resume_text (str): Full resume text.\n",
    "        job_text (str): Combined job description and requirements text.\n",
    "\n",
    "    Returns:\n",
    "        float: Skill match score between 0 and 1.\n",
    "               Returns 0 if no skills were found in the job text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFjvXoZZATIt"
   },
   "outputs": [],
   "source": [
    "def calculate_skills_score(resume_text, job_text):\n",
    "    resume_lower = resume_text.lower()\n",
    "    job_lower = job_text.lower()\n",
    "    job_skills = [skill for skill in skills if skill in job_lower]\n",
    "    if not job_skills:\n",
    "        return 0\n",
    "    resume_skills = [skill for skill in job_skills if skill in resume_lower]\n",
    "    return len(resume_skills) / len(job_skills)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DutM5lzLA1tO"
   },
   "source": [
    "    Estimates the candidate's experience level from their resume.\n",
    "\n",
    "    Steps:\n",
    "    1. Converts text to lowercase.\n",
    "    2. Searches for any mention of \"X years\" to estimate years of experience.\n",
    "    3. Counts seniority-related keywords like 'manager', 'senior', etc.\n",
    "    4. Combines both measures into a normalized score:\n",
    "       (years / 10) + (experience_keywords / 5), capped at 1.0.\n",
    "\n",
    "    Args:\n",
    "        resume_text (str): Full resume text.\n",
    "\n",
    "    Returns:\n",
    "        float: Experience score between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhtyOkEXATlo"
   },
   "outputs": [],
   "source": [
    "def calculate_experience_score(resume_text):\n",
    "    text_lower = resume_text.lower()\n",
    "    years_matches = re.findall(r'(\\d+)\\s*(?:years?|yrs?)', text_lower)\n",
    "    max_years = max([int(year) for year in years_matches]) if years_matches else 0\n",
    "    exp_count = sum(1 for word in experience_words if word in text_lower)\n",
    "    return min((max_years / 10) + (exp_count / 5), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw5oxYhWA5Tg"
   },
   "source": [
    "    Determines the highest education level mentioned in the resume.\n",
    "\n",
    "    Steps:\n",
    "    1. Converts resume to lowercase.\n",
    "    2. Checks for mentions of education keywords ('phd', 'master', etc.).\n",
    "    3. Maps the highest degree found to a numeric value from education_levels.\n",
    "    4. Normalizes by dividing by 4 (the highest possible score).\n",
    "\n",
    "    Args:\n",
    "        resume_text (str): Full resume text.\n",
    "\n",
    "    Returns:\n",
    "        float: Education score between 0 and 1.\n",
    "               Higher degrees produce higher scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ2nV0oWAToO"
   },
   "outputs": [],
   "source": [
    "def calculate_education_score(resume_text):\n",
    "    text_lower = resume_text.lower()\n",
    "    max_education = 0\n",
    "    for level, score in education_levels.items():\n",
    "        if level in text_lower:\n",
    "            max_education = max(max_education, score)\n",
    "    return min(max_education / 4, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5_JCwNOA9QC"
   },
   "source": [
    "    Evaluates whether the resume and job post belong to the same domain/industry.\n",
    "\n",
    "    Steps:\n",
    "    1. Identifies the job's domain (HR, IT, finance, etc.) based on domain_keywords.\n",
    "    2. Counts how many domain-specific keywords appear in the job description.\n",
    "    3. Checks how many of those same keywords appear in the resume.\n",
    "    4. Returns the ratio of matching domain keywords.\n",
    "       If the job domain cannot be determined, returns a neutral 0.5.\n",
    "\n",
    "    Args:\n",
    "        resume_text (str): Resume text.\n",
    "        job_text (str): Job posting text.\n",
    "\n",
    "    Returns:\n",
    "        float: Domain relevance score between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBQNhEu4ATqk"
   },
   "outputs": [],
   "source": [
    "def calculate_domain_score(resume_text, job_text):\n",
    "    resume_lower = resume_text.lower()\n",
    "    job_lower = job_text.lower()\n",
    "    job_domain = 'general'\n",
    "    max_domain_score = 0\n",
    "    for domain, keywords in domain_keywords.items():\n",
    "        domain_score = sum(1 for keyword in keywords if keyword in job_lower)\n",
    "        if domain_score > max_domain_score:\n",
    "            max_domain_score = domain_score\n",
    "            job_domain = domain\n",
    "    if job_domain == 'general':\n",
    "        return 0.5\n",
    "    domain_keywords_list = domain_keywords[job_domain]\n",
    "    matches = sum(1 for keyword in domain_keywords_list if keyword in resume_lower)\n",
    "    return min(matches / len(domain_keywords_list), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHgTCKwKBHjq"
   },
   "source": [
    "    Calculates the overall match score between a resume and a job posting\n",
    "    by combining all five sub-scores using predefined feature weights.\n",
    "\n",
    "    Steps:\n",
    "    1. Calls each of the five scoring functions:\n",
    "         - calculate_skills_score\n",
    "         - calculate_experience_score\n",
    "         - calculate_education_score\n",
    "         - calculate_domain_score\n",
    "         - calculate_semantic_score\n",
    "    2. Stores each sub-score in a dictionary for transparency.\n",
    "    3. Computes a weighted average using the 'weights' dictionary defined earlier.\n",
    "    4. Returns both the final composite score and the individual component scores.\n",
    "\n",
    "    Formula:\n",
    "        final_score = Σ (weight_i × score_i)\n",
    "        where i ∈ {skills, experience, education, domain, semantic}\n",
    "\n",
    "    Args:\n",
    "        resume_text (str): Full resume text.\n",
    "        job_text (str): Combined job description and requirements text.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - final_score (float): Overall weighted fit score (0–1 range).\n",
    "            - scores (dict): Dictionary of component scores for analysis and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjp1EDARUdba"
   },
   "outputs": [],
   "source": [
    "#final score\n",
    "def calculate_composite_score(resume_text, job_text):\n",
    "    scores = {\n",
    "        'skills': calculate_skills_score(resume_text, job_text),\n",
    "        'experience': calculate_experience_score(resume_text),\n",
    "        'education': calculate_education_score(resume_text),\n",
    "        'domain': calculate_domain_score(resume_text, job_text),\n",
    "        'semantic': calculate_semantic_score(resume_text, job_text)\n",
    "    }\n",
    "    final_score = sum(weights[component] * scores[component] for component in scores.keys())\n",
    "    return final_score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T16HY7pbVzy9"
   },
   "outputs": [],
   "source": [
    "# #job text field\n",
    "# job_posts_df['job_text'] = job_posts_df.apply(\n",
    "#     lambda row: ' '.join([str(row[field]) for field in ['Title', 'JobDescription', 'JobRequirment', 'RequiredQual'] if pd.notna(row[field])]),\n",
    "#     axis=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qpgVVXJGbO7"
   },
   "source": [
    "GENERATE TRAINING PAIRS: JOB POSTINGS ↔ TOP-MATCHING RESUMES\n",
    "\n",
    "Purpose:\n",
    "This section builds the annotated dataset that pairs each job posting\n",
    "with its top matching resumes, using the composite scoring system.\n",
    "\n",
    "Process:\n",
    "1. Takes a sample of job postings (first 100 for efficiency).\n",
    "2. For each job posting:\n",
    "   • Retrieves the combined job text (title, description, requirements, etc.)\n",
    "   • Iterates through every resume in the dataset.\n",
    "   • Uses calculate_composite_score() to compute a weighted “fit score”\n",
    "     based on skills, experience, education, domain, and semantic similarity.\n",
    "3. Stores each (job, resume) pair along with its detailed component scores.\n",
    "4. Sorts the resumes by their final score and keeps only the top 10 matches per job.\n",
    "5. Appends all top results into a single DataFrame called training_pairs.\n",
    "\n",
    "Outcome:\n",
    "A structured dataset containing job–resume pairs, ranked by relevance scores,\n",
    "which will later be used for labeling and potential supervised model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "3LFA6kgIV54U",
    "outputId": "d26ebd1e-d7af-4083-904d-badb73a94b40"
   },
   "outputs": [],
   "source": [
    "# generate training pairs\n",
    "training_data = []\n",
    "job_sample = job_posts_df.head(100)\n",
    "\n",
    "for job_idx, job_row in job_sample.iterrows():\n",
    "    job_text = job_row['job_text']\n",
    "    job_title = job_row['Title']\n",
    "    resume_scores = []\n",
    "\n",
    "    for resume_idx, resume_row in resume_df.iterrows():\n",
    "        resume_text = resume_row['Resume_str']\n",
    "        resume_id = resume_row['ID']\n",
    "        final_score, component_scores = calculate_composite_score(resume_text, job_text)\n",
    "        resume_scores.append({\n",
    "            'job_idx': job_idx,\n",
    "            'resume_idx': resume_idx,\n",
    "            'job_title': job_title,\n",
    "            'resume_id': resume_id,\n",
    "            'final_score': final_score,\n",
    "            **component_scores\n",
    "        })\n",
    "\n",
    "    # sort and keep top 10\n",
    "    resume_scores.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "    training_data.extend(resume_scores[:10])\n",
    "\n",
    "    if job_idx % 25 == 0:\n",
    "        print(f\"Processed {job_idx + 1} jobs...\")\n",
    "\n",
    "training_pairs = pd.DataFrame(training_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4cheIAPGtVz"
   },
   "source": [
    "\"\"\"\n",
    "LABEL GENERATION: CONVERT CONTINUOUS SCORES INTO CATEGORICAL CLASSES\n",
    "\n",
    "Purpose:\n",
    "This section transforms the continuous \"final_score\" values into discrete\n",
    "labels (1 = good fit, 0 = poor fit) to enable supervised learning or\n",
    "evaluation later on.\n",
    "\n",
    "Process:\n",
    "1. Extracts all final composite scores from the training_pairs DataFrame.\n",
    "2. Calculates score thresholds using quartiles:\n",
    "   • High threshold (75th percentile) → top-performing resumes.\n",
    "   • Low threshold (25th percentile) → least suitable resumes.\n",
    "3. Assigns labels based on these cutoffs:\n",
    "   • 1  → score ≥ high_threshold (good fit)\n",
    "   • 0  → score ≤ low_threshold (poor fit)\n",
    "   • -1 → scores in the middle range (ambiguous, excluded)\n",
    "4. Filters out all -1 entries to keep only clearly positive and negative examples.\n",
    "\n",
    "Outcome:\n",
    "Creates labeled_training_data — a balanced dataset of strong and weak\n",
    "resume–job matches suitable for training or evaluating future models.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_o8PW_5XV__d"
   },
   "outputs": [],
   "source": [
    "#labels\n",
    "scores = training_pairs['final_score']\n",
    "high_threshold = scores.quantile(0.75)\n",
    "low_threshold = scores.quantile(0.25)\n",
    "\n",
    "print(f\"Score thresholds: High={high_threshold:.3f}, Low={low_threshold:.3f}\")\n",
    "\n",
    "labels = []\n",
    "for score in scores:\n",
    "    if score >= high_threshold:\n",
    "        labels.append(1)\n",
    "    elif score <= low_threshold:\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(-1)\n",
    "\n",
    "training_pairs['label'] = labels\n",
    "labeled_training_data = training_pairs[training_pairs['label'] != -1].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lujdX6CnWJe7"
   },
   "outputs": [],
   "source": [
    "# annotated dataset\n",
    "output_file = 'resume_job_training_data.csv'\n",
    "labeled_training_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9bFxLn8Hzky"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(resume_df[\"label\"], bins=30)\n",
    "plt.title(\"Distribution of Resume–Job Fit Scores\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_n6zBnVK-aX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
