# ğŸ¨ Visual Project Structure

Quick visual reference for understanding the project organization.

---

## ğŸ“Š **High-Level Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         YOUR PROJECT                             â”‚
â”‚                    AI Resume-Job Matching                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DATA       â”‚    â”‚  NOTEBOOKS   â”‚    â”‚   MODELS     â”‚
â”‚  (Storage)   â”‚    â”‚  (Analysis)  â”‚    â”‚  (Trained)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                     â”‚
        â”‚                     â”‚                     â”‚
        â–¼                     â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRECOMPUTED  â”‚    â”‚    UTILS     â”‚    â”‚ PRODUCTION   â”‚
â”‚ (Fast Load)  â”‚    â”‚  (Helpers)   â”‚    â”‚   (eval.py)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—‚ï¸ **Folder Hierarchy with Purpose**

```
ğŸ“¦ ai-recruitment/
â”‚
â”œâ”€â”€ ğŸ“‚ data/                          â† ALL YOUR DATA LIVES HERE
â”‚   â”œâ”€â”€ ğŸ“‚ original/                  â† Raw & cleaned CSVs
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ resumes.csv           (2,484 resumes - raw)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ resumes_cleaned.csv   (2,484 resumes - clean) âœ…
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ jobs.csv              (5,448 jobs - raw)
â”‚   â”‚   â””â”€â”€ ğŸ“„ jobs_cleaned.csv      (5,448 jobs - clean) âœ…
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ embeddings/                â† Precomputed vectors
â”‚   â”‚   â”œâ”€â”€ ğŸ”¢ resume_emb_e5_large.npy      (BEST - 1024-dim) â­
â”‚   â”‚   â”œâ”€â”€ ğŸ”¢ job_emb_e5_large.npy         (BEST - 1024-dim) â­
â”‚   â”‚   â”œâ”€â”€ ğŸ”¢ resume_emb_all_mpnet.npy     (768-dim)
â”‚   â”‚   â”œâ”€â”€ ğŸ”¢ job_emb_all_mpnet.npy        (768-dim)
â”‚   â”‚   â”œâ”€â”€ ğŸ”¢ resume_emb_multi_qa_mpnet.npy (768-dim)
â”‚   â”‚   â”œâ”€â”€ ğŸ”¢ job_emb_multi_qa_mpnet.npy    (768-dim)
â”‚   â”‚   â”œâ”€â”€ ğŸ”¢ resume_emb_all-MiniLM.npy    (384-dim, FAST) ğŸš€
â”‚   â”‚   â””â”€â”€ ğŸ”¢ job_emb_all-MiniLM.npy       (384-dim, FAST) ğŸš€
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“„ train_pairs_rich.csv       â† Training data (115K pairs)
â”‚   â””â”€â”€ ğŸ“„ test_pairs_rich.csv        â† Test data (20K pairs)
â”‚
â”œâ”€â”€ ğŸ“‚ models/                         â† TRAINED ML MODELS
â”‚   â”œâ”€â”€ ğŸ¤– model_random_forest.pkl    (Main model) â­
â”‚   â”œâ”€â”€ ğŸ¤– model_logreg.pkl           (Baseline)
â”‚   â”œâ”€â”€ ğŸ¤– model_gradient_boosting.pkl (Best accuracy)
â”‚   â”œâ”€â”€ ğŸ¤– model_svm_(rbf).pkl        (Experimental)
â”‚   â””â”€â”€ ğŸ¤– model_neural_network.pkl   (Deep learning)
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/                      â† YOUR MAIN WORKSPACE
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ““ PIPELINE (Run in order 1â†’6)
â”‚   â”‚   â”œâ”€â”€ 1ï¸âƒ£ 1preprocessing.ipynb          (Clean data)
â”‚   â”‚   â”œâ”€â”€ 2ï¸âƒ£ 2feature_extraction.ipynb     (Generate embeddings)
â”‚   â”‚   â”œâ”€â”€ 3ï¸âƒ£ 3cosine_sim.ipynb             (Compute similarity)
â”‚   â”‚   â”œâ”€â”€ 4ï¸âƒ£ 4similarity_diagnostics.ipynb (Analyze distributions)
â”‚   â”‚   â”œâ”€â”€ 5ï¸âƒ£ 5evaluation.ipynb             (Clustering eval)
â”‚   â”‚   â””â”€â”€ 6ï¸âƒ£ gpt.ipynb                     (Train ML models) â­
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ”¬ MODEL COMPARISON
â”‚   â”‚   â”œâ”€â”€ ğŸ“Š model_comparison.py           (Compare 5 models) â­
â”‚   â”‚   â”œâ”€â”€ ğŸ¯ hyperparameter_tuning.py      (Optimize models)
â”‚   â”‚   â””â”€â”€ ğŸ““ 6model_comparison.ipynb       (Interactive)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸš€ PRODUCTION
â”‚   â”‚   â”œâ”€â”€ âš¡ eval.py                       (Real-time ranking) â­
â”‚   â”‚   â”œâ”€â”€ ğŸ’¾ precompute_embeddings.py      (Precompute resumes)
â”‚   â”‚   â””â”€â”€ ğŸ› ï¸ feature_extractors.py         (Feature utilities)
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ¨ Poppins-Medium.ttf                (Font for plots)
â”‚
â”œâ”€â”€ ğŸ“‚ precomputed/                    â† PRODUCTION-READY DATA
â”‚   â”œâ”€â”€ ğŸ”¢ resume_vectors.npy         (Fast loading)
â”‚   â”œâ”€â”€ ğŸ“„ resume_texts.csv           (Cleaned texts)
â”‚   â””â”€â”€ ğŸ“‹ resume_index.json          (Filename mapping)
â”‚
â”œâ”€â”€ ğŸ“‚ utils/                          â† HELPER FUNCTIONS
â”‚   â””â”€â”€ ğŸ› ï¸ utils.py                   (Text preprocessing)
â”‚
â”œâ”€â”€ ğŸ“‚ oldnotebooks/                   â† ARCHIVE (experiments)
â”‚   â””â”€â”€ ğŸ““ [9 old notebooks]          (Not used in main pipeline)
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt                â† Python dependencies
â”œâ”€â”€ ğŸ“„ README.md                       â† Project overview
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md            â† This guide! ğŸ“–
â””â”€â”€ ğŸ“„ .gitignore                      â† Git rules
```

---

## ğŸ”„ **Data Flow: From Raw Data to Production**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: DATA PREPARATION                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“¥ Kaggle Dataset
         â”‚
         â”œâ”€ resumes.csv (2,484 resumes)
         â””â”€ jobs.csv (5,448 jobs)
         â”‚
         â–¼
    [1preprocessing.ipynb]
    â€¢ Remove HTML
    â€¢ Handle missing values
    â€¢ Remove duplicates
    â€¢ Text preprocessing
         â”‚
         â–¼
    ğŸ“¤ data/original/
         â”œâ”€ resumes_cleaned.csv âœ…
         â””â”€ jobs_cleaned.csv âœ…

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: EMBEDDING GENERATION                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“¥ resumes_cleaned.csv + jobs_cleaned.csv
         â”‚
         â–¼
    [2feature_extraction.ipynb]
    â€¢ Load SentenceTransformer
    â€¢ Encode all texts
    â€¢ Test 4 different models
    â€¢ Normalize embeddings
         â”‚
         â–¼
    ğŸ“¤ data/embeddings/
         â”œâ”€ resume_emb_e5_large.npy â­ (BEST)
         â”œâ”€ job_emb_e5_large.npy â­
         â””â”€ [6 other embedding files]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: TRAINING DATA GENERATION                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“¥ Embeddings + Cleaned CSVs
         â”‚
         â–¼
    [gpt.ipynb]
    â€¢ Split jobs 85/15
    â€¢ Generate pairs (job Ã— resume)
    â€¢ Use cosine sim for labels
    â€¢ Extract 5 features
    â€¢ Create train/test sets
         â”‚
         â–¼
    ğŸ“¤ data/
         â”œâ”€ train_pairs_rich.csv (115,775 pairs)
         â””â”€ test_pairs_rich.csv (20,425 pairs)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 4: MODEL TRAINING                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“¥ train_pairs_rich.csv
         â”‚
         â–¼
    [gpt.ipynb] OR [model_comparison.py]
    â€¢ Train Random Forest
    â€¢ Train Logistic Regression
    â€¢ Train Gradient Boosting
    â€¢ Train SVM
    â€¢ Train Neural Network
    â€¢ Evaluate all models
         â”‚
         â–¼
    ğŸ“¤ models/
         â”œâ”€ model_random_forest.pkl â­
         â”œâ”€ model_logreg.pkl
         â”œâ”€ model_gradient_boosting.pkl
         â”œâ”€ model_svm_(rbf).pkl
         â””â”€ model_neural_network.pkl

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 5: PRODUCTION DEPLOYMENT                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“¥ Real resumes (*.docx files)
         â”‚
         â–¼
    [precompute_embeddings.py]
    â€¢ Load all resumes
    â€¢ Clean text
    â€¢ Embed with e5-large-v2
    â€¢ Save for fast loading
         â”‚
         â–¼
    ğŸ“¤ precomputed/
         â”œâ”€ resume_vectors.npy
         â”œâ”€ resume_texts.csv
         â””â”€ resume_index.json
         â”‚
         â–¼
    [eval.py] â† PRODUCTION INFERENCE
    â€¢ Load precomputed embeddings
    â€¢ Embed new job posting
    â€¢ Compute cosine similarity
    â€¢ Extract ML features
    â€¢ Combine signals
    â€¢ Return top-K resumes
         â”‚
         â–¼
    ğŸ“¤ Top-10 Ranked Resumes ğŸ¯
```

---

## ğŸ¯ **File Importance Matrix**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    IMPORTANCE LEVELS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â­â­â­ CRITICAL - Must have for system to work              â”‚
â”‚ â­â­  IMPORTANT - Needed for full functionality             â”‚
â”‚ â­   OPTIONAL - Nice to have, not essential                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DATA FILES:
â­â­â­ resumes_cleaned.csv          (Core dataset)
â­â­â­ jobs_cleaned.csv             (Core dataset)
â­â­â­ resume_emb_e5_large.npy      (Best embeddings)
â­â­â­ job_emb_e5_large.npy         (Best embeddings)
â­â­  train_pairs_rich.csv         (Training data)
â­â­  test_pairs_rich.csv          (Evaluation data)
â­   Other embedding files         (Alternative models)

NOTEBOOKS:
â­â­â­ gpt.ipynb                    (Main training)
â­â­â­ eval.py                      (Production inference)
â­â­  model_comparison.py          (Model selection)
â­â­  1preprocessing.ipynb         (Data cleaning)
â­â­  2feature_extraction.ipynb    (Embedding generation)
â­â­  feature_extractors.py        (Utilities)
â­   3cosine_sim.ipynb            (Analysis)
â­   4similarity_diagnostics.ipynb (Analysis)
â­   5evaluation.ipynb            (Analysis)
â­   precompute_embeddings.py     (Production prep)
â­   hyperparameter_tuning.py     (Optimization)

MODELS:
â­â­â­ model_random_forest.pkl      (Main production model)
â­â­  model_gradient_boosting.pkl  (Best accuracy)
â­   model_logreg.pkl             (Fast baseline)
â­   Other models                  (Experimental)

UTILITIES:
â­â­  feature_extractors.py        (Feature functions)
â­   utils.py                      (Text preprocessing)
```

---

## ğŸš¦ **Quick Start Paths**

### **Path 1: I want to understand the system**
```
1. Read README.md
2. Open gpt.ipynb
3. Look at eval.py
4. Check model_comparison.py
```

### **Path 2: I want to train models**
```
1. Run 1preprocessing.ipynb
2. Run 2feature_extraction.ipynb
3. Run gpt.ipynb
4. Run model_comparison.py
```

### **Path 3: I want to deploy to production**
```
1. Collect real resumes (*.docx)
2. Run precompute_embeddings.py
3. Use eval.py for inference
4. Load best model from models/
```

### **Path 4: I want to improve the system**
```
1. Modify feature_extractors.py (add features)
2. Retrain with gpt.ipynb
3. Compare with model_comparison.py
4. Tune with hyperparameter_tuning.py
```

---

## ğŸ“ **Size Reference**

```
SMALL FILES (<1 MB):
â”œâ”€â”€ All .py scripts
â”œâ”€â”€ All .md documentation
â””â”€â”€ requirements.txt

MEDIUM FILES (1-100 MB):
â”œâ”€â”€ resumes_cleaned.csv (~50 MB)
â”œâ”€â”€ jobs_cleaned.csv (~10 MB)
â”œâ”€â”€ train_pairs_rich.csv (~30 MB)
â”œâ”€â”€ test_pairs_rich.csv (~5 MB)
â””â”€â”€ Most .pkl models (~10-50 MB each)

LARGE FILES (>100 MB):
â”œâ”€â”€ All .npy embedding files (~100-200 MB each)
â”œâ”€â”€ model_random_forest.pkl (~100 MB)
â””â”€â”€ precomputed/resume_vectors.npy (~500 MB)

TOTAL PROJECT: ~2-3 GB
```

---

## ğŸ“ **For Presentations/Interviews**

**Show this structure when explaining your project:**

```
"My project has 3 main components:

1ï¸âƒ£ DATA PIPELINE (notebooks 1-2)
   â†’ Clean data and generate embeddings

2ï¸âƒ£ TRAINING PIPELINE (gpt.ipynb + model_comparison.py)
   â†’ Train and compare ML models

3ï¸âƒ£ PRODUCTION SYSTEM (eval.py + precomputed/)
   â†’ Real-time ranking with precomputed embeddings

The key innovation is the hybrid approach:
combining semantic similarity (embeddings) with
rule-based features (skills, education) for
better ranking quality."
```

---

**Questions?** See `PROJECT_STRUCTURE.md` for detailed explanations!
