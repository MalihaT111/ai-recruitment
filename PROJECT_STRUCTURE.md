# ğŸ“ AI-Assisted Resume-Job Matching: Project Structure

Complete guide to understanding the file organization and purpose of each component.

---

## ğŸŒ³ Directory Tree Overview

```
ai-recruitment/
â”œâ”€â”€ ğŸ“‚ data/                    # All datasets and generated data
â”œâ”€â”€ ğŸ“‚ models/                  # Trained ML models
â”œâ”€â”€ ğŸ“‚ notebooks/               # Jupyter notebooks & scripts (main work)
â”œâ”€â”€ ğŸ“‚ precomputed/             # Production-ready precomputed data
â”œâ”€â”€ ğŸ“‚ utils/                   # Utility functions
â”œâ”€â”€ ğŸ“‚ oldnotebooks/            # Archive of experimental notebooks
â”œâ”€â”€ ğŸ“„ requirements.txt         # Python dependencies
â”œâ”€â”€ ğŸ“„ README.md               # Project overview
â””â”€â”€ ğŸ“„ .gitignore              # Git ignore rules
```

---

## ğŸ“‚ **1. `/data/` - All Datasets**

### **Purpose:** Store raw, cleaned, and processed datasets

```
data/
â”œâ”€â”€ original/                   # Raw and cleaned source data
â”‚   â”œâ”€â”€ resumes.csv            # Original 2,484 resumes (Kaggle)
â”‚   â”œâ”€â”€ resumes_cleaned.csv    # Cleaned resumes (HTML removed, preprocessed)
â”‚   â”œâ”€â”€ jobs.csv               # Original 5,448 job postings (Kaggle)
â”‚   â””â”€â”€ jobs_cleaned.csv       # Cleaned jobs (combined fields, preprocessed)
â”‚
â”œâ”€â”€ embeddings/                 # Precomputed embeddings (multiple models)
â”‚   â”œâ”€â”€ resume_emb_e5_large.npy          # e5-large-v2 (1024-dim, BEST)
â”‚   â”œâ”€â”€ job_emb_e5_large.npy             # e5-large-v2 job embeddings
â”‚   â”œâ”€â”€ resume_emb_all_mpnet.npy         # all-mpnet-base-v2 (768-dim)
â”‚   â”œâ”€â”€ job_emb_all_mpnet.npy            # all-mpnet-base-v2 jobs
â”‚   â”œâ”€â”€ resume_emb_multi_qa_mpnet.npy    # multi-qa-mpnet (768-dim)
â”‚   â”œâ”€â”€ job_emb_multi_qa_mpnet.npy       # multi-qa-mpnet jobs
â”‚   â”œâ”€â”€ resume_emb_all-MiniLM.npy        # all-MiniLM-L6-v2 (384-dim, FAST)
â”‚   â””â”€â”€ job_emb_all-MiniLM.npy           # all-MiniLM-L6-v2 jobs
â”‚
â”œâ”€â”€ train_pairs_rich.csv        # Training pairs (115,775 pairs)
â”‚   # Columns: job_id, resume_id, cosine_sim, keyword_overlap,
â”‚   #          skill_score, education_score, experience_score,
â”‚   #          domain_match, hybrid_score, label
â”‚
â””â”€â”€ test_pairs_rich.csv         # Test pairs (20,425 pairs)
    # Same structure as train_pairs_rich.csv
```

**Key Files:**
- **`resumes_cleaned.csv`**: 2,484 resumes with `Resume_clean` column
- **`jobs_cleaned.csv`**: 5,448 jobs with `job_text_clean` column
- **`*_emb_e5_large.npy`**: Production embeddings (best quality)
- **`train_pairs_rich.csv`**: 85% of jobs Ã— 25 resumes each
- **`test_pairs_rich.csv`**: 15% of jobs Ã— 25 resumes each

---

## ğŸ“‚ **2. `/models/` - Trained ML Models**

### **Purpose:** Store trained model files for production use

```
models/
â”œâ”€â”€ model_random_forest.pkl          # Random Forest (from gpt.ipynb)
â”œâ”€â”€ model_logreg.pkl                 # Logistic Regression (from gpt.ipynb)
â”œâ”€â”€ model_gradient_boosting.pkl      # Gradient Boosting (from comparison)
â”œâ”€â”€ model_svm_(rbf).pkl              # SVM with RBF kernel
â”œâ”€â”€ model_neural_network.pkl         # Multi-layer Perceptron
â”œâ”€â”€ model_rf_tuned.pkl               # Tuned Random Forest (optional)
â””â”€â”€ model_gb_tuned.pkl               # Tuned Gradient Boosting (optional)
```

**How to Load:**
```python
import joblib
model = joblib.load("models/model_random_forest.pkl")
```

**Model Sizes:**
- Logistic Regression: ~10 KB (smallest)
- Random Forest: ~50-100 MB
- Gradient Boosting: ~20-50 MB
- Neural Network: ~5-10 MB

---

## ğŸ“‚ **3. `/notebooks/` - Main Development Work**

### **Purpose:** All analysis, training, and evaluation code

```
notebooks/
â”œâ”€â”€ ğŸ““ CORE PIPELINE (Run in order)
â”‚   â”œâ”€â”€ 1preprocessing.ipynb              # Step 1: Clean raw data
â”‚   â”œâ”€â”€ 2feature_extraction.ipynb         # Step 2: Generate embeddings
â”‚   â”œâ”€â”€ 3cosine_sim.ipynb                 # Step 3: Compute similarity
â”‚   â”œâ”€â”€ 4similarity_diagnostics.ipynb     # Step 4: Analyze distributions
â”‚   â”œâ”€â”€ 5evaluation.ipynb                 # Step 5: Unsupervised eval
â”‚   â””â”€â”€ gpt.ipynb                         # Step 6: Train ML models
â”‚
â”œâ”€â”€ ğŸ“Š MODEL COMPARISON (New!)
â”‚   â”œâ”€â”€ model_comparison.py               # Compare 5 ML models
â”‚   â”œâ”€â”€ hyperparameter_tuning.py          # Tune best models
â”‚   â””â”€â”€ 6model_comparison.ipynb           # Interactive comparison
â”‚
â”œâ”€â”€ ğŸš€ PRODUCTION CODE
â”‚   â”œâ”€â”€ eval.py                           # Real-time ranking inference
â”‚   â”œâ”€â”€ precompute_embeddings.py          # Precompute resume embeddings
â”‚   â””â”€â”€ feature_extractors.py            # Feature extraction utilities
â”‚
â”œâ”€â”€ ğŸ“š DOCUMENTATION
â”‚   â”œâ”€â”€ 6_model_comparison.md             # Model comparison guide
â”‚   â””â”€â”€ MODEL_COMPARISON_GUIDE.md         # Detailed guide
â”‚
â””â”€â”€ ğŸ¨ ASSETS
    â””â”€â”€ Poppins-Medium.ttf                # Font for visualizations
```

### **Detailed File Descriptions:**

#### **Core Pipeline Notebooks:**

**`1preprocessing.ipynb`** - Data Cleaning
- Removes HTML tags from resumes
- Handles missing values
- Removes duplicates
- Combines job description fields
- Applies text preprocessing (lowercase, stopwords, lemmatization)
- **Output:** `resumes_cleaned.csv`, `jobs_cleaned.csv`

**`2feature_extraction.ipynb`** - Embedding Generation
- Loads cleaned data
- Uses SentenceTransformer to generate embeddings
- Tests multiple models (e5-large, mpnet, MiniLM)
- Normalizes embeddings (L2 norm)
- **Output:** `*_emb_*.npy` files in `data/embeddings/`

**`3cosine_sim.ipynb`** - Similarity Analysis
- Computes cosine similarity matrix (5448 Ã— 2484)
- Shows top-K matches for sample jobs
- Implements cross-encoder reranking
- Demonstrates pure semantic matching
- **Output:** Similarity visualizations

**`4similarity_diagnostics.ipynb`** - Distribution Analysis
- Analyzes similarity score distributions
- Computes timing benchmarks
- Generates histograms and statistics
- **Output:** Diagnostic plots

**`5evaluation.ipynb`** - Unsupervised Evaluation
- K-Means clustering on combined embeddings
- Computes clustering metrics (Silhouette, Calinski-Harabasz, Davies-Bouldin)
- Analyzes resume-job mixing in clusters
- PCA visualization (2D projection)
- **Output:** Clustering quality metrics

**`gpt.ipynb`** - ML Model Training (MAIN TRAINING NOTEBOOK)
- Splits jobs 85/15 (train/test)
- Generates training pairs with pseudo-labels
- Extracts 5 features (keyword_overlap, skill_score, etc.)
- Trains Random Forest + Logistic Regression
- Evaluates with ranking metrics (P@10, R@10, MAP@10, NDCG@10)
- **Output:** `train_pairs_rich.csv`, `test_pairs_rich.csv`, trained models

---

#### **Model Comparison Scripts:**

**`model_comparison.py`** - Compare Multiple Models
```python
# Compares 5 models:
# 1. Logistic Regression
# 2. Random Forest
# 3. Gradient Boosting
# 4. SVM (RBF)
# 5. Neural Network (MLP)

# Evaluates on:
# - Classification: ROC-AUC, Avg Precision
# - Ranking: P@10, R@10, MAP@10, NDCG@10

# Outputs:
# - CSV results table
# - 3 visualization plots
# - All trained models
```

**`hyperparameter_tuning.py`** - Optimize Best Models
```python
# Uses GridSearchCV to tune:
# - Random Forest (n_estimators, max_depth, etc.)
# - Gradient Boosting (learning_rate, n_estimators, etc.)

# Outputs:
# - model_rf_tuned.pkl
# - model_gb_tuned.pkl
# - hyperparameter_tuning_results.csv
```

---

#### **Production Code:**

**`eval.py`** - Real-Time Ranking
```python
# Production inference script
# Usage:
#   from eval import rank_resumes
#   results = rank_resumes(job_text, top_k=10, alpha=0.7)

# Features:
# - Loads precomputed resume embeddings
# - Embeds new job posting
# - Computes cosine similarity
# - Extracts ML features
# - Combines signals (Î±Ã—ML + (1-Î±)Ã—cosine)
# - Returns top-K ranked resumes
```

**`precompute_embeddings.py`** - Precompute Resume Embeddings
```python
# For production deployment
# Reads .docx resumes from /Resumes/ folder
# Cleans and embeds all resumes
# Saves to /precomputed/ for fast inference

# Outputs:
# - resume_vectors.npy (embeddings)
# - resume_texts.csv (cleaned text)
# - resume_index.json (filename mapping)
```

**`feature_extractors.py`** - Feature Utilities
```python
# Shared utility functions:
# - clean_text_for_domain()
# - detect_domain()
# - skill_match()
# - education_score()
# - seniority_score()
# - keyword_overlap()

# Used by: gpt.ipynb, eval.py, model_comparison.py
```

---

## ğŸ“‚ **4. `/precomputed/` - Production Data**

### **Purpose:** Fast inference with precomputed embeddings

```
precomputed/
â”œâ”€â”€ resume_vectors.npy          # Precomputed embeddings (N Ã— 1024)
â”œâ”€â”€ resume_texts.csv            # Cleaned resume texts
â””â”€â”€ resume_index.json           # Filename â†’ index mapping
```

**How it works:**
1. Run `precompute_embeddings.py` once
2. Loads all resumes from `/Resumes/*.docx`
3. Embeds them with e5-large-v2
4. Saves to `/precomputed/`
5. `eval.py` loads these for fast inference

**Benefit:** Only need to embed job posting at runtime (not all resumes)

---

## ğŸ“‚ **5. `/utils/` - Utility Functions**

### **Purpose:** Shared helper functions

```
utils/
â””â”€â”€ utils.py                    # Text preprocessing utilities
    # Functions:
    # - preprocess_text()      # Lowercase, remove stopwords, lemmatize
    # - clean_html()           # Remove HTML tags
    # - tokenize()             # Word tokenization
```

**Used by:** `1preprocessing.ipynb`

---

## ğŸ“‚ **6. `/oldnotebooks/` - Archive**

### **Purpose:** Experimental notebooks (not part of main pipeline)

```
oldnotebooks/
â”œâ”€â”€ 3edanormal.ipynb                    # Early EDA
â”œâ”€â”€ 4edaembedding.ipynb                 # Embedding experiments
â”œâ”€â”€ 5clustering.ipynb                   # Clustering attempts
â”œâ”€â”€ 6clustering.ipynb                   # More clustering
â”œâ”€â”€ 7clustering.ipynb                   # Even more clustering
â”œâ”€â”€ 8trainingset.ipynb                  # Training set experiments
â”œâ”€â”€ 9trainingset.ipynb                  # More training experiments
â”œâ”€â”€ AI_Assisted_Recruitment.ipynb       # Original prototype
â””â”€â”€ mali_AI_Assisted_Recruitment.ipynb  # Another prototype
```

**Note:** These are kept for reference but not used in the main pipeline.

---

## ğŸ“„ **Root Files**

### **`requirements.txt`** - Python Dependencies
```txt
numpy
pandas
scikit-learn
sentence-transformers
torch
transformers
nltk
matplotlib
seaborn
jupyter
joblib
python-docx
tqdm
```

### **`README.md`** - Project Overview
- High-level description
- Installation instructions
- Quick start guide
- Results summary

### **`.gitignore`** - Git Ignore Rules
```
__pycache__/
*.pyc
.DS_Store
*.pkl
*.npy
data/original/*.csv
precomputed/
```

---

## ğŸ”„ **Data Flow Diagram**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PREPARATION                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
    resumes.csv, jobs.csv (Kaggle)
                            â†“
    [1preprocessing.ipynb] â†’ resumes_cleaned.csv, jobs_cleaned.csv
                            â†“
    [2feature_extraction.ipynb] â†’ resume_emb_*.npy, job_emb_*.npy
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRAINING PIPELINE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
    [gpt.ipynb]
    â”œâ”€ Split jobs 85/15
    â”œâ”€ Generate pairs with labels
    â”œâ”€ Extract features
    â”œâ”€ Train RF + LR
    â””â”€ Evaluate
                            â†“
    train_pairs_rich.csv, test_pairs_rich.csv
    model_random_forest.pkl, model_logreg.pkl
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL COMPARISON                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
    [model_comparison.py]
    â”œâ”€ Train 5 models
    â”œâ”€ Evaluate all
    â””â”€ Generate plots
                            â†“
    model_*.pkl (5 models)
    model_comparison_results.csv
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRODUCTION DEPLOYMENT                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
    [precompute_embeddings.py] â†’ /precomputed/
                            â†“
    [eval.py] â†’ Real-time ranking
```

---

## ğŸ¯ **Quick Navigation Guide**

### **I want to...**

| Goal | File to Use |
|------|-------------|
| Clean raw data | `notebooks/1preprocessing.ipynb` |
| Generate embeddings | `notebooks/2feature_extraction.ipynb` |
| Train ML models | `notebooks/gpt.ipynb` |
| Compare models | `notebooks/model_comparison.py` |
| Rank resumes in production | `notebooks/eval.py` |
| Precompute embeddings | `notebooks/precompute_embeddings.py` |
| Understand features | `notebooks/feature_extractors.py` |
| See results | `data/model_comparison_results.csv` |
| Load trained model | `models/model_random_forest.pkl` |

---

## ğŸ“Š **File Size Reference**

```
Total Project Size: ~2-3 GB

Breakdown:
â”œâ”€â”€ data/embeddings/        ~1.5 GB  (8 embedding files)
â”œâ”€â”€ data/original/          ~50 MB   (CSV files)
â”œâ”€â”€ data/train_pairs.csv    ~30 MB
â”œâ”€â”€ data/test_pairs.csv     ~5 MB
â”œâ”€â”€ models/                 ~200 MB  (all models)
â”œâ”€â”€ precomputed/            ~500 MB  (production embeddings)
â””â”€â”€ notebooks/              ~100 MB  (notebooks + outputs)
```

---

## ğŸš€ **Recommended Workflow**

### **For New Users:**
1. Read `README.md`
2. Install dependencies: `pip install -r requirements.txt`
3. Run notebooks 1-5 in order
4. Run `gpt.ipynb` to train models
5. Run `model_comparison.py` to compare models

### **For Production Deployment:**
1. Run `precompute_embeddings.py` on your resume database
2. Use `eval.py` for real-time ranking
3. Load best model from `models/`

### **For Experimentation:**
1. Modify `feature_extractors.py` to add features
2. Retrain with `gpt.ipynb`
3. Compare with `model_comparison.py`

---

**Questions?** Check the README or individual file docstrings!
