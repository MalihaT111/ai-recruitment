{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HxO60AMK_dx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiaTSuo_AHHe",
    "outputId": "ab0bea20-2ff5-4e5a-a4f4-1bd13f407bd5"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IfsMtzY2dsB"
   },
   "source": [
    "All of our Google Drive files look the same, so that's where we are pulling our data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb4RXPXfA4Ps",
    "outputId": "1e3a00a5-60f3-40d6-e43a-d7c6f541284a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# data_path = \"/content/drive/MyDrive/cadence 1a/data\"\n",
    "# print(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnQKUhFH2lld"
   },
   "source": [
    "We read the resume dataset and do some preliminary analysis to see howw it looks. We look at the null values, the number of columns and rows, as well as the sum of null values. We proceed to do the same for the job posts dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UiC5YeKgBUKp",
    "outputId": "b5d21c31-9a39-4f5c-d980-75f01b43dd9a"
   },
   "outputs": [],
   "source": [
    "# resume_df = pd.read_csv(f\"{data_path}/resumes.csv\")\n",
    "resume_df = pd.read_csv(\"resumes.csv\")\n",
    "resume_df.shape\n",
    "resume_df.head()\n",
    "# resume_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "igUkm9NdRWQA",
    "outputId": "d2470e6a-5ff5-476e-b8b2-90beb0afccd4"
   },
   "outputs": [],
   "source": [
    "# job_posts_df = pd.read_csv(f\"{data_path}/jobs.csv\")\n",
    "job_posts_df = pd.read_csv(\"jobs.csv\")\n",
    "job_posts_df.shape\n",
    "job_posts_df.head()\n",
    "# job_posts_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcHnyhf9221a"
   },
   "source": [
    "Columns that have missing values in the job posts dataset are being turned into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2E757sJxRuaV",
    "outputId": "2676decd-72f2-4d07-93fe-40faebe6ce75"
   },
   "outputs": [],
   "source": [
    "condition = job_posts_df.isnull().sum() != 0\n",
    "job_posts_df.isnull().sum()[condition].index\n",
    "columnlist = list(job_posts_df.isnull().sum()[condition].index)\n",
    "columnlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhBidoMS3NFU"
   },
   "source": [
    "There are no numerical values in this data set; everything is stored in string/object format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "URFmCMdoXRBC",
    "outputId": "412ac215-58d7-4e99-8c0f-7a74a20afdee"
   },
   "outputs": [],
   "source": [
    "job_posts_df[columnlist].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gVcVpRye9Dp"
   },
   "source": [
    "## Addressing Null values in job_posts_df\n",
    "Since there are many columns in job_posts_df with null values, we can use reasoning to drop some of the rows or columns. Some columns can be cut if they have too many null values or if they are not really relevant to the problem.  For example, the column \"AnnouncementCode\" has 17793 null values and the unique non-null values are a random string of letters. It was most likely used to identify the job posting on its original website. As well, \"Opening Date\" and \"Deadline\" may not have many null values, but these values might not be very useful to determine whether or not a candidate would be a good fit for a job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "QACcPPTBaAiY",
    "outputId": "bd03e616-205d-49c5-e124-3ab62f3566bf"
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['AnnouncementCode', 'Term', 'Eligibility', 'Audience', 'StartDate', 'Duration', 'OpeningDate', 'Deadline', 'Notes', 'Attach']\n",
    "job_posts_df = job_posts_df.drop(columns=columns_to_drop)\n",
    "job_posts_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2ZAjOMuiirN"
   },
   "source": [
    "There are still null values, but the columns are too contextually important to the ML problem. We can drop the examples that have null values in these columns since they most likely do not have the information we need to train the model accurately. After we drop these rows, our job_posts_df dataset no longer has any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83OGw9jigbBQ",
    "outputId": "c4f0afd9-0b87-4abc-aac7-c9b0fffd4ff2"
   },
   "outputs": [],
   "source": [
    "columns_to_check = ['Title', 'JobDescription', 'JobRequirment', 'Company', 'Location', 'RequiredQual', 'Salary', 'AboutC']\n",
    "\n",
    "job_posts_df = job_posts_df.dropna(subset=columns_to_check)\n",
    "\n",
    "job_posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "jTtLm_A4hk8J",
    "outputId": "2bae69b1-4307-4e43-c354-8e4cd06512b7"
   },
   "outputs": [],
   "source": [
    "job_posts_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tB5bFRjO3sL6"
   },
   "source": [
    "Removing duplicates from both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjVSiapQqOyl",
    "outputId": "70219f57-1e82-4eb4-a63a-f38606887ba4"
   },
   "outputs": [],
   "source": [
    "job_posts_df = job_posts_df.drop_duplicates()\n",
    "print(job_posts_df.duplicated().sum())\n",
    "resume_df = resume_df.drop_duplicates()\n",
    "print(resume_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjgA3eMW3xbm"
   },
   "source": [
    "Finding columns with HTML tags. No column in job post has any HTML tags we need to remove. Only resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FOg5qmAsLF0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def has_html(text):\n",
    "    if isinstance(text, str):\n",
    "        return bool(re.search(r'<.*?>', text))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZduWLCTspLd",
    "outputId": "25e550fb-f374-407b-b153-a4b98b2247d4"
   },
   "outputs": [],
   "source": [
    "columns_with_html = [col for col in job_posts_df.columns if job_posts_df[col].apply(has_html).any()]\n",
    "print(columns_with_html)\n",
    "\n",
    "columns_with_html = [col for col in resume_df.columns if resume_df[col].apply(has_html).any()]\n",
    "print(columns_with_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmPWzI_frHOD"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_html(text):\n",
    "  return re.sub('<[^<]+?>', '', text)\n",
    "\n",
    "resume_df['Resume_html'] = resume_df['Resume_str'].apply(clean_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byjNEVv43-NG"
   },
   "source": [
    "Resume_html and Resume_str are the same column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhN8VCwlry0E"
   },
   "outputs": [],
   "source": [
    "resume_df[['Resume_html', 'Resume_str']].head()\n",
    "resume_df.drop(columns=['Resume_html'], inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgMICFDzEng_"
   },
   "source": [
    "Creates a corpus for all job post related details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5V8QYCpECSN"
   },
   "outputs": [],
   "source": [
    "job_posts_df[\"job_text\"] = (\n",
    "    \"Description: \" + job_posts_df[\"JobDescription\"].fillna('') + \" \"\n",
    "    \"Requirements: \" + job_posts_df[\"JobRequirment\"].fillna('') + \" \"\n",
    "    \"Qualifications: \" + job_posts_df[\"RequiredQual\"].fillna('') + \" \"\n",
    "    \"About Company: \" + job_posts_df[\"AboutC\"].fillna('')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scu-RU6EGFQa"
   },
   "source": [
    "# Checkpoint #2 - Text Normalization\n",
    "Apply tokenization, lowercasing, stopword removal, and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OzDqZxH8BFd"
   },
   "source": [
    "Import NLP and text-processing tools:\n",
    "- nltk for natural language processing utilities (downloads WordNet for lemmatization)\n",
    "- TfidfVectorizer and ENGLISH_STOP_WORDS from sklearn to convert text into numerical features and remove common stop words\n",
    "- WordNetLemmatizer to reduce words to their base (dictionary) form\n",
    "- word_tokenize: splits sentences into individual words\n",
    "- stopwords: provides common words (like \"the\", \"and\") to remove from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06Rgh2QGQTL",
    "outputId": "ba3e4075-b101-450c-a410-907f4694d2cd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTKrkD358l2T"
   },
   "source": [
    "Make sure NLTK resources are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmMAYi1c8oLR",
    "outputId": "e596a60c-0689-42b8-83d6-0c0d15c44119"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFbtCv-s9jwA"
   },
   "source": [
    "Initialize the WordNet lemmatizer and define a set of English stopwords for text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_uCAasx9fIL"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0--N5H6fB56Z"
   },
   "source": [
    "\n",
    " TEXT PREPROCESSING PIPELINE FOR RESUMES AND JOB POSTS\n",
    "\n",
    " Purpose:\n",
    "   This section standardizes and cleans all text data to prepare it\n",
    "   for accurate keyword and semantic matching in the scoring system.\n",
    "\n",
    " Description:\n",
    "   The preprocess_text() function normalizes text by performing:\n",
    "     1. Lowercasing – ensures consistent word comparisons.\n",
    "     2. Tokenization – splits text into individual words.\n",
    "     3. Stopword removal – removes common filler words like \"the\", \"and\", \"is\".\n",
    "     4. Filtering – keeps only alphabetic tokens (drops numbers/punctuation).\n",
    "     5. Lemmatization – reduces words to their base form\n",
    "        (e.g., “running” → “run”, “analyses” → “analysis”).\n",
    "     6. Reconstruction – joins cleaned tokens back into a single string.\n",
    "\n",
    "   This preprocessing is applied to:\n",
    "     • resume_df[\"Resume_str\"]  → creates resume_df[\"Resume_clean\"]\n",
    "     • job_posts_df[\"job_text\"] → creates job_posts_df[\"job_text_clean\"]\n",
    "\n",
    " Outcome:\n",
    "   Produces standardized, lemmatized, and stopword-free text columns\n",
    "   for both resumes and job postings, enabling fair and consistent#   matching in later scoring functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7wo8FRRB4ZQ"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Lowercase, tokenize, remove stopwords, and lemmatize.\"\"\"\n",
    "    tokens = word_tokenize(str(text).lower())\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "resume_df[\"Resume_clean\"] = resume_df[\"Resume_str\"].apply(preprocess_text)\n",
    "job_posts_df['job_text_clean'] = job_posts_df['job_text'].apply(preprocess_text)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "resume_embeddings = model.encode(resume_df[\"Resume_clean\"].tolist(), show_progress_bar=True)\n",
    "job_embeddings = model.encode(job_posts_df[\"job_text_clean\"].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "resume_lengths = resume_df['Resume_str'].str.len()\n",
    "job_lengths = job_posts_df['JobDescription'].str.len()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.hist(resume_lengths, bins=50, alpha=0.7)\n",
    "ax1.set_title('Resume Text Length Distribution')\n",
    "ax1.set_xlabel('Character Count')\n",
    "\n",
    "ax2.hist(job_lengths, bins=50, alpha=0.7)\n",
    "ax2.set_title('Job Description Length Distribution')\n",
    "ax2.set_xlabel('Character Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Get most common words in resumes and job postings\n",
    "def get_top_words(text_series, n=45):\n",
    "    all_words = ' '.join(text_series).lower().split()\n",
    "    # Remove stopwords and short words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in all_words if word not in stop_words and len(word) > 2]\n",
    "    return Counter(filtered_words).most_common(n)\n",
    "\n",
    "top_resume_words = get_top_words(resume_df['Resume_str'])\n",
    "top_job_words = get_top_words(job_posts_df['JobDescription'])\n",
    "print(top_resume_words)\n",
    "print(top_job_words)\n",
    "# Plot word frequencies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "words, counts = zip(*top_resume_words)\n",
    "ax1.barh(words, counts)\n",
    "ax1.set_title('Top Words in Resumes')\n",
    "\n",
    "words, counts = zip(*top_job_words)\n",
    "ax2.barh(words, counts)\n",
    "ax2.set_title('Top Words in Job Descriptions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common skills extraction (simplified)\n",
    "# Add/update list with keywords we are interested in\n",
    "skills_keywords = ['python', 'java', 'sql', 'machine learning', 'aws',\n",
    "                   'docker', 'kubernetes', 'react', 'node.js', 'tensorflow']\n",
    "\n",
    "def count_skills(text, skills_list):\n",
    "    text_lower = text.lower()\n",
    "    return sum(1 for skill in skills_list if skill in text_lower)\n",
    "\n",
    "# Count skills in resumes and job postings\n",
    "for skill in skills_keywords:\n",
    "    resume_df[f'resume_has_{skill}'] = resume_df['Resume_str'].str.lower().str.contains(skill)\n",
    "    job_posts_df[f'job_has_{skill}'] = job_posts_df['JobDescription'].str.lower().str.contains(skill)\n",
    "\n",
    "# Plot skills frequency\n",
    "resume_skills_count = resume_df[[f'resume_has_{skill}' for skill in skills_keywords]].sum()\n",
    "job_skills_count = job_posts_df[[f'job_has_{skill}' for skill in skills_keywords]].sum()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "resume_skills_count.plot(kind='barh', ax=ax1)\n",
    "ax1.set_title('Skills Frequency in Resumes')\n",
    "job_skills_count.plot(kind='barh', ax=ax2)\n",
    "ax2.set_title('Skills Frequency in Job Postings')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Bigram analysis\n",
    "def plot_top_ngrams(text_series, n=2, top_k=45):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english',\n",
    "                               max_features=top_k)\n",
    "    X = vectorizer.fit_transform(text_series)\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    counts = X.sum(axis=0).A1\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(words, counts)\n",
    "    plt.title(f'Top {n}-grams')\n",
    "    plt.show()\n",
    "\n",
    "# Compare bigrams in resumes vs job postings\n",
    "plot_top_ngrams(resume_df['Resume_str'], n=2)\n",
    "plot_top_ngrams(job_posts_df['JobDescription'], n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iZWtf3XTGxk"
   },
   "source": [
    "# Checkpoint #3 - Data Annotation\n",
    "\n",
    "define weights for: skills, experience, education, semantic similarity, domain\n",
    "creates labels (good match/top 25% = 1, bad match/bottom 25% = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22PQECrK_4Bo"
   },
   "source": [
    "\n",
    "# SCORING CONFIGURATION AND KEYWORD DEFINITIONS\n",
    "\n",
    " This section defines:\n",
    " 1. Feature weights — how much each category contributes to the final composite score.\n",
    " 2. Domain-specific keywords — used to detect the job domain (e.g., HR, finance, IT).\n",
    " 3. Common skill keywords — used to evaluate skill overlap between job and resume.\n",
    "4. Education levels — mapped to numeric values for scoring academic background.\n",
    " 5. Experience indicators — keywords representing seniority or management experience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7D65CJJAxuR"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ckQe9XcTrU_"
   },
   "outputs": [],
   "source": [
    "#define weights\n",
    "weights = {\n",
    "    'skills': 0.35,\n",
    "    'experience': 0.20,\n",
    "    'education': 0.15,\n",
    "    'semantic': 0.15,\n",
    "    'domain': 0.15\n",
    "}\n",
    "\n",
    "\n",
    "domain_keywords = {\n",
    "    'hr': [\n",
    "        'human resources', 'hr', 'recruitment', 'recruiting', 'hiring',\n",
    "        'payroll', 'benefits', 'employee relations', 'compensation',\n",
    "        'performance management', 'talent acquisition', 'training',\n",
    "        'onboarding', 'diversity', 'compliance', 'employee engagement',\n",
    "        'career development', 'hr policies', 'conflict resolution',\n",
    "        'organizational development', 'leadership', 'communication skills',\n",
    "        'workplace safety', 'microsoft office', 'workforce planning',\n",
    "        'hr analytics', 'labor law', 'employee retention'\n",
    "    ],\n",
    "\n",
    "    'finance': [\n",
    "        'finance', 'financial', 'accounting', 'budget', 'budgeting', 'audit', 'tax',\n",
    "        'bookkeeping', 'financial analysis', 'forecasting', 'financial modeling',\n",
    "        'cash flow', 'profit', 'loss', 'ledger', 'accounts payable',\n",
    "        'accounts receivable', 'payable', 'receivable', 'valuation',\n",
    "        'cost analysis', 'financial reporting', 'economics', 'treasury',\n",
    "        'capital markets', 'credit', 'debit', 'banking', 'investment',\n",
    "        'excel', 'power bi', 'data analysis'\n",
    "    ],\n",
    "\n",
    "    'it': [\n",
    "        'programming', 'software', 'development', 'software development',\n",
    "        'software engineer', 'python', 'java', 'sql', 'database', 'web development',\n",
    "        'network', 'system administration', 'cloud', 'aws', 'azure', 'gcp',\n",
    "        'devops', 'docker', 'kubernetes', 'linux', 'git', 'version control',\n",
    "        'testing', 'debugging', 'api', 'backend', 'frontend', 'node.js', 'react',\n",
    "        'data science', 'machine learning', 'tensorflow', 'automation',\n",
    "        'cybersecurity'\n",
    "    ],\n",
    "\n",
    "    'sales': [\n",
    "        'sales', 'business development', 'account management', 'revenue', 'crm',\n",
    "        'client', 'customer', 'lead generation', 'cold calling', 'prospecting',\n",
    "        'presentation', 'closing deals', 'negotiation', 'pipeline', 'quota',\n",
    "        'target', 'territory', 'upselling', 'cross-selling', 'b2b', 'b2c',\n",
    "        'account executive', 'retail', 'merchandising', 'promotion', 'marketing',\n",
    "        'sales strategy', 'partnerships', 'client relations', 'relationship management',\n",
    "        'sales operations', 'business partnerships'\n",
    "    ],\n",
    "\n",
    "    'administration': [\n",
    "        'administrative', 'secretary', 'assistant', 'coordination', 'office',\n",
    "        'organization', 'communication', 'customer service', 'documentation',\n",
    "        'inventory', 'scheduling', 'calendar management', 'data entry', 'filing',\n",
    "        'record keeping', 'reception', 'travel arrangements', 'correspondence',\n",
    "        'procurement', 'clerical', 'executive assistant', 'meeting planning',\n",
    "        'office management', 'support staff', 'event planning', 'vendor management',\n",
    "        'front desk', 'logistics', 'supplies', 'records management',\n",
    "        'budget tracking', 'document control', 'office coordination',\n",
    "        'front office', 'mail management'\n",
    "    ],\n",
    "\n",
    "    'research': [\n",
    "        'research', 'analyst', 'analysis', 'data analysis', 'methodology',\n",
    "        'report', 'evaluation', 'literature review', 'hypothesis', 'experiment',\n",
    "        'survey', 'study', 'quantitative', 'qualitative', 'statistics', 'modeling',\n",
    "        'scientific', 'investigation', 'findings', 'insight', 'insights',\n",
    "        'publication', 'predictive modeling', 'data visualization',\n",
    "        'policy analysis', 'impact assessment', 'data collection', 'r',\n",
    "        'spss', 'tableau', 'power bi'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "skills = ['excel', 'word', 'powerpoint', 'sql', 'python', 'project management', 'data analysis', 'ms office', 'microsoft office']\n",
    "\n",
    "education_levels = {'phd': 4, 'master': 3, 'bachelor': 2, 'associate': 1, 'diploma': 0.5}\n",
    "\n",
    "experience_words = ['manager', 'director', 'senior', 'lead', 'specialist', 'analyst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_keywords_in_series(text_series, domain_keywords):\n",
    "    \"\"\"Counts all domain keywords in a pandas Series of text efficiently.\"\"\"\n",
    "    domain_counts = {domain: 0 for domain in domain_keywords}\n",
    "    keyword_counts = Counter()\n",
    "    \n",
    "    for domain, keywords in domain_keywords.items():\n",
    "        for kw in keywords:\n",
    "            # Build a regex for full word or phrase match (case-insensitive)\n",
    "            pattern = rf'\\b{re.escape(kw.lower())}\\b'\n",
    "            # Sum counts across all rows\n",
    "            count = text_series.str.count(pattern, flags=re.IGNORECASE).sum()\n",
    "            if count > 0:\n",
    "                domain_counts[domain] += count\n",
    "                keyword_counts[(domain, kw)] += count\n",
    "                \n",
    "    return domain_counts, keyword_counts\n",
    "\n",
    "resume_domain_counts, resume_kw_counts = count_keywords_in_series(resume_df['Resume_str'], domain_keywords)\n",
    "job_domain_counts, job_kw_counts = count_keywords_in_series(job_posts_df['job_text'], domain_keywords)\n",
    "\n",
    "\n",
    "domain_summary = pd.DataFrame({\n",
    "    'domain': list(domain_keywords.keys()),\n",
    "    'resume_keyword_count': [resume_domain_counts[d] for d in domain_keywords],\n",
    "    'job_keyword_count': [job_domain_counts[d] for d in domain_keywords]\n",
    "}).sort_values(by='resume_keyword_count', ascending=False)\n",
    "\n",
    "# Optional: detailed keyword-level breakdown\n",
    "resume_kw_df = pd.DataFrame(resume_kw_counts.items(), columns=['(domain, keyword)', 'resume_count'])\n",
    "job_kw_df = pd.DataFrame(job_kw_counts.items(), columns=['(domain, keyword)', 'job_count'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 10 keywords by count\n",
    "print(\"=== Resume Keyword Counts (Top 30) ===\")\n",
    "display(resume_kw_df.sort_values('resume_count', ascending=False).head(30))\n",
    "\n",
    "print(\"=== Job Keyword Counts (Top 30) ===\")\n",
    "display(job_kw_df.sort_values('job_count', ascending=False).head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== Domain-Level Keyword Summary ===\")\n",
    "print(domain_summary, \"\\n\")\n",
    "\n",
    "print(\"=== Sample Keyword-Level Breakdown (Top 10) ===\")\n",
    "print(resume_kw_df.sort_values('resume_count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_compare = pd.merge(\n",
    "    resume_kw_df, job_kw_df,\n",
    "    on='(domain, keyword)', how='outer'\n",
    ").fillna(0)\n",
    "\n",
    "kw_compare['difference'] = kw_compare['resume_count'] - kw_compare['job_count']\n",
    "\n",
    "# Show top 20 overrepresented in resumes\n",
    "kw_compare.sort_values('difference', ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eA1zSCsAfOs"
   },
   "source": [
    "Computes a basic semantic similarity between a resume and job posting.\n",
    "\n",
    "Steps:\n",
    "1. Converts both texts to lowercase.\n",
    "2. Splits them into sets of unique words.\n",
    "3. Finds the overlap (intersection) between the two word sets.\n",
    "4. Returns the proportion of job words also present in the resume.\n",
    "\n",
    "Note:\n",
    "    This is a simple lexical overlap metric, not a deep semantic one.\n",
    "    It can later be replaced with embedding-based cosine similarity.\n",
    "\n",
    "Args:\n",
    "    resume_text (str): Resume text.\n",
    "    job_text (str): Job posting text.\n",
    "\n",
    "Returns:\n",
    "    float: Overlap ratio between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0tKyuA_UQZn"
   },
   "outputs": [],
   "source": [
    "def calculate_semantic_score(resume_text, job_text):\n",
    "    resume_words = set(resume_text.lower().split())\n",
    "    job_words = set(job_text.lower().split())\n",
    "    if not job_words:\n",
    "        return 0\n",
    "    overlap = len(resume_words.intersection(job_words))\n",
    "    return overlap / len(job_words) if overlap > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMZB0lqbAkvh"
   },
   "source": [
    "Calculates how well a resume matches the skills required by a job posting.\n",
    "\n",
    "Steps:\n",
    "1. Converts both resume and job descriptions to lowercase.\n",
    "2. Extracts all skill keywords that appear in the job text.\n",
    "3. Checks which of those required skills also appear in the resume.\n",
    "4. Returns the ratio of matched skills to total job-required skills.\n",
    "\n",
    "Args:\n",
    "    resume_text (str): Full resume text.\n",
    "    job_text (str): Combined job description and requirements text.\n",
    "\n",
    "Returns:\n",
    "    float: Skill match score between 0 and 1.\n",
    "            Returns 0 if no skills were found in the job text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFjvXoZZATIt"
   },
   "outputs": [],
   "source": [
    "def calculate_skills_score(resume_text, job_text):\n",
    "    resume_lower = resume_text.lower()\n",
    "    job_lower = job_text.lower()\n",
    "    job_skills = [skill for skill in skills if skill in job_lower]\n",
    "    if not job_skills:\n",
    "        return 0\n",
    "    resume_skills = [skill for skill in job_skills if skill in resume_lower]\n",
    "    return len(resume_skills) / len(job_skills)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DutM5lzLA1tO"
   },
   "source": [
    "Estimates the candidate's experience level from their resume.\n",
    "\n",
    "Steps:\n",
    "1. Converts text to lowercase.\n",
    "2. Searches for any mention of \"X years\" to estimate years of experience.\n",
    "3. Counts seniority-related keywords like 'manager', 'senior', etc.\n",
    "4. Combines both measures into a normalized score:\n",
    "    (years / 10) + (experience_keywords / 5), capped at 1.0.\n",
    "\n",
    "Args:\n",
    "    resume_text (str): Full resume text.\n",
    "\n",
    "Returns:\n",
    "    float: Experience score between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhtyOkEXATlo"
   },
   "outputs": [],
   "source": [
    "def calculate_experience_score(resume_text):\n",
    "    text_lower = resume_text.lower()\n",
    "    years_matches = re.findall(r'(\\d+)\\s*(?:years?|yrs?)', text_lower)\n",
    "    max_years = max([int(year) for year in years_matches]) if years_matches else 0\n",
    "    exp_count = sum(1 for word in experience_words if word in text_lower)\n",
    "    return min((max_years / 10) + (exp_count / 5), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw5oxYhWA5Tg"
   },
   "source": [
    "Determines the highest education level mentioned in the resume.\n",
    "\n",
    "Steps:\n",
    "1. Converts resume to lowercase.\n",
    "2. Checks for mentions of education keywords ('phd', 'master', etc.).\n",
    "3. Maps the highest degree found to a numeric value from education_levels.\n",
    "4. Normalizes by dividing by 4 (the highest possible score).\n",
    "\n",
    "Args:\n",
    "    resume_text (str): Full resume text.\n",
    "\n",
    "Returns:\n",
    "    float: Education score between 0 and 1.\n",
    "            Higher degrees produce higher scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ2nV0oWAToO"
   },
   "outputs": [],
   "source": [
    "def calculate_education_score(resume_text):\n",
    "    text_lower = resume_text.lower()\n",
    "    max_education = 0\n",
    "    for level, score in education_levels.items():\n",
    "        if level in text_lower:\n",
    "            max_education = max(max_education, score)\n",
    "    return min(max_education / 4, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5_JCwNOA9QC"
   },
   "source": [
    "Evaluates whether the resume and job post belong to the same domain/industry.\n",
    "\n",
    "Steps:\n",
    "1. Identifies the job's domain (HR, IT, finance, etc.) based on domain_keywords.\n",
    "2. Counts how many domain-specific keywords appear in the job description.\n",
    "3. Checks how many of those same keywords appear in the resume.\n",
    "4. Returns the ratio of matching domain keywords.\n",
    "    If the job domain cannot be determined, returns a neutral 0.5.\n",
    "\n",
    "Args:\n",
    "    resume_text (str): Resume text.\n",
    "    job_text (str): Job posting text.\n",
    "\n",
    "Returns:\n",
    "    float: Domain relevance score between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBQNhEu4ATqk"
   },
   "outputs": [],
   "source": [
    "def calculate_domain_score(resume_text, job_text):\n",
    "    resume_lower = resume_text.lower()\n",
    "    job_lower = job_text.lower()\n",
    "    job_domain = 'general'\n",
    "    max_domain_score = 0\n",
    "    for domain, keywords in domain_keywords.items():\n",
    "        domain_score = sum(1 for keyword in keywords if keyword in job_lower)\n",
    "        if domain_score > max_domain_score:\n",
    "            max_domain_score = domain_score\n",
    "            job_domain = domain\n",
    "    if job_domain == 'general':\n",
    "        return 0.5\n",
    "    domain_keywords_list = domain_keywords[job_domain]\n",
    "    matches = sum(1 for keyword in domain_keywords_list if keyword in resume_lower)\n",
    "    return min(matches / len(domain_keywords_list), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHgTCKwKBHjq"
   },
   "source": [
    "Calculates the overall match score between a resume and a job posting\n",
    "by combining all five sub-scores using predefined feature weights.\n",
    "\n",
    "Steps:\n",
    "1. Calls each of the five scoring functions:\n",
    "        - calculate_skills_score\n",
    "        - calculate_experience_score\n",
    "        - calculate_education_score\n",
    "        - calculate_domain_score\n",
    "        - calculate_semantic_score\n",
    "2. Stores each sub-score in a dictionary for transparency.\n",
    "3. Computes a weighted average using the 'weights' dictionary defined earlier.\n",
    "4. Returns both the final composite score and the individual component scores.\n",
    "\n",
    "Formula:\n",
    "    final_score = Σ (weight_i × score_i)\n",
    "    where i ∈ {skills, experience, education, domain, semantic}\n",
    "\n",
    "Args:\n",
    "    resume_text (str): Full resume text.\n",
    "    job_text (str): Combined job description and requirements text.\n",
    "\n",
    "Returns:\n",
    "    tuple:\n",
    "        - final_score (float): Overall weighted fit score (0–1 range).\n",
    "        - scores (dict): Dictionary of component scores for analysis and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjp1EDARUdba"
   },
   "outputs": [],
   "source": [
    "#final score\n",
    "def calculate_composite_score(resume_text, job_text):\n",
    "    scores = {\n",
    "        'skills': calculate_skills_score(resume_text, job_text),\n",
    "        'experience': calculate_experience_score(resume_text),\n",
    "        'education': calculate_education_score(resume_text),\n",
    "        'domain': calculate_domain_score(resume_text, job_text),\n",
    "        'semantic': calculate_semantic_score(resume_text, job_text)\n",
    "    }\n",
    "    final_score = sum(weights[component] * scores[component] for component in scores.keys())\n",
    "    return final_score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- Resume Feature Extraction -----\n",
    "def get_resume_features(resume_text):\n",
    "    return {\n",
    "        'skills': calculate_skills_score(resume_text, \"\"),\n",
    "        'experience': calculate_experience_score(resume_text),\n",
    "        'education': calculate_education_score(resume_text),\n",
    "        'domain': calculate_domain_score(resume_text, \"\")\n",
    "    }\n",
    "\n",
    "resume_features = []\n",
    "for _, row in resume_df.iterrows():\n",
    "    features = get_resume_features(row['Resume_str'])\n",
    "    resume_features.append({'resume_id': row['ID'], **features})\n",
    "\n",
    "resume_features_df = pd.DataFrame(resume_features)\n",
    "\n",
    "\n",
    "# ----- Job Feature Extraction -----\n",
    "def get_job_features(job_text):\n",
    "    return {\n",
    "        'skills': calculate_skills_score(job_text, \"\"),\n",
    "        'experience': calculate_experience_score(job_text),\n",
    "        'education': calculate_education_score(job_text),\n",
    "        'domain': calculate_domain_score(job_text, \"\")\n",
    "    }\n",
    "\n",
    "job_features = []\n",
    "for _, row in job_posts_df.iterrows():\n",
    "    features = get_job_features(row['job_text'])\n",
    "    job_features.append({'job_id': row.get('ID', _), 'title': row['Title'], **features})\n",
    "\n",
    "job_features_df = pd.DataFrame(job_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# --- Detect domain of each job post ---\n",
    "def detect_domain(text, domain_keywords):\n",
    "    text_lower = text.lower()\n",
    "    scores = {domain: sum(kw in text_lower for kw in keywords) for domain, keywords in domain_keywords.items()}\n",
    "    return max(scores, key=scores.get) if max(scores.values()) > 0 else \"other\"\n",
    "\n",
    "job_posts_df[\"domain\"] = job_posts_df[\"job_text_clean\"].apply(lambda x: detect_domain(x, domain_keywords))\n",
    "\n",
    "# --- Cluster jobs by domain using K-Means ---\n",
    "domain_clusters = {}\n",
    "job_posts_df[\"cluster\"] = -1  # default value\n",
    "\n",
    "for domain, group in job_posts_df.groupby(\"domain\"):\n",
    "    if len(group) < 3:\n",
    "        print(f\"Skipping domain '{domain}' (too few samples: {len(group)})\")\n",
    "        continue\n",
    "\n",
    "    embeddings_subset = job_embeddings[group.index.tolist()]\n",
    "\n",
    "    # heuristic: choose cluster count based on group size (between 2 and 8)\n",
    "    n_clusters = max(2, min(8, len(group) // 10))  # e.g., 1 cluster per ~10 jobs\n",
    "    print(f\"Clustering domain '{domain}' with n_clusters={n_clusters}\")\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "    cluster_labels = kmeans.fit_predict(embeddings_subset)\n",
    "\n",
    "    job_posts_df.loc[group.index, \"cluster\"] = cluster_labels\n",
    "    domain_clusters[domain] = kmeans\n",
    "\n",
    "print(\"✅ Domain-based K-Means clustering complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_clusters = cluster_entities_dbscan(resume_features_df, \"Resume\", eps=0.4, min_samples=3)\n",
    "job_clusters = cluster_entities_dbscan(job_features_df, \"Job\", eps=0.45, min_samples=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.pairplot(resume_features_df[['skills','experience','education','domain']])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_clusters.groupby('cluster')[['skills','experience','education','domain']].mean()\n",
    "job_clusters.groupby('cluster')[['skills','experience','education','domain']].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qpgVVXJGbO7"
   },
   "source": [
    "GENERATE TRAINING PAIRS: JOB POSTINGS ↔ TOP-MATCHING RESUMES\n",
    "\n",
    "Purpose:\n",
    "This section builds the annotated dataset that pairs each job posting\n",
    "with its top matching resumes, using the composite scoring system.\n",
    "\n",
    "Process:\n",
    "1. Takes a sample of job postings (first 100 for efficiency).\n",
    "2. For each job posting:\n",
    "   • Retrieves the combined job text (title, description, requirements, etc.)\n",
    "   • Iterates through every resume in the dataset.\n",
    "   • Uses calculate_composite_score() to compute a weighted “fit score”\n",
    "     based on skills, experience, education, domain, and semantic similarity.\n",
    "3. Stores each (job, resume) pair along with its detailed component scores.\n",
    "4. Sorts the resumes by their final score and keeps only the top 10 matches per job.\n",
    "5. Appends all top results into a single DataFrame called training_pairs.\n",
    "\n",
    "Outcome:\n",
    "A structured dataset containing job–resume pairs, ranked by relevance scores,\n",
    "which will later be used for labeling and potential supervised model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# def process_one_job(job_idx, job_row, resume_df, calculate_composite_score):\n",
    "#     job_title = job_row['Title']\n",
    "#     job_text = job_row['job_text']\n",
    "\n",
    "#     resume_results = []\n",
    "\n",
    "#     for resume_idx, resume_row in resume_df.iterrows():\n",
    "#         resume_text = resume_row['Resume_str']\n",
    "#         resume_id = resume_row['ID']\n",
    "#         final_score, component_scores = calculate_composite_score(resume_text, job_text)\n",
    "        \n",
    "#         resume_results.append({\n",
    "#             'job_idx': job_idx,\n",
    "#             'resume_idx': resume_idx,\n",
    "#             'job_title': job_title,\n",
    "#             'resume_id': resume_id,\n",
    "#             'final_score': final_score,\n",
    "#             **component_scores  # Include component breakdowns\n",
    "#         })\n",
    "\n",
    "#     # Pick top 10 efficiently\n",
    "#     resume_results.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "#     return resume_results[:10]\n",
    "\n",
    "\n",
    "# def generate_training_pairs_parallel(job_posts_df, resume_df, calculate_composite_score):\n",
    "#     job_sample = job_posts_df\n",
    "    \n",
    "#     training_data = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "#         delayed(process_one_job)(idx, row, resume_df, calculate_composite_score)\n",
    "#         for idx, row in job_sample.iterrows()\n",
    "#     )\n",
    "\n",
    "#     training_data_flat = [x for sublist in training_data for x in sublist]\n",
    "#     training_pairs = pd.DataFrame(training_data_flat)\n",
    "\n",
    "#     print(f\"Processed {len(job_sample)} jobs in parallel.\")\n",
    "#     return training_pairs\n",
    "\n",
    "\n",
    "# # Run it\n",
    "# training_pairs = generate_training_pairs_parallel(job_posts_df, resume_df, calculate_composite_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4cheIAPGtVz"
   },
   "source": [
    "\"\"\"\n",
    "LABEL GENERATION: CONVERT CONTINUOUS SCORES INTO CATEGORICAL CLASSES\n",
    "\n",
    "Purpose:\n",
    "This section transforms the continuous \"final_score\" values into discrete\n",
    "labels (1 = good fit, 0 = poor fit) to enable supervised learning or\n",
    "evaluation later on.\n",
    "\n",
    "Process:\n",
    "1. Extracts all final composite scores from the training_pairs DataFrame.\n",
    "2. Calculates score thresholds using quartiles:\n",
    "   • High threshold (75th percentile) → top-performing resumes.\n",
    "   • Low threshold (25th percentile) → least suitable resumes.\n",
    "3. Assigns labels based on these cutoffs:\n",
    "   • 1  → score ≥ high_threshold (good fit)\n",
    "   • 0  → score ≤ low_threshold (poor fit)\n",
    "   • -1 → scores in the middle range (ambiguous, excluded)\n",
    "4. Filters out all -1 entries to keep only clearly positive and negative examples.\n",
    "\n",
    "Outcome:\n",
    "Creates labeled_training_data — a balanced dataset of strong and weak\n",
    "resume–job matches suitable for training or evaluating future models.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_o8PW_5XV__d"
   },
   "outputs": [],
   "source": [
    "# #labels\n",
    "# scores = training_pairs['final_score']\n",
    "# high_threshold = scores.quantile(0.75)\n",
    "# low_threshold = scores.quantile(0.25)\n",
    "\n",
    "# print(f\"Score thresholds: High={high_threshold:.3f}, Low={low_threshold:.3f}\")\n",
    "\n",
    "# labels = []\n",
    "# for score in scores:\n",
    "#     if score >= high_threshold:\n",
    "#         labels.append(1)\n",
    "#     elif score <= low_threshold:\n",
    "#         labels.append(0)\n",
    "#     else:\n",
    "#         labels.append(-1)\n",
    "\n",
    "# training_pairs['label'] = labels\n",
    "# labeled_training_data = training_pairs[training_pairs['label'] != -1].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lujdX6CnWJe7"
   },
   "outputs": [],
   "source": [
    "# # annotated dataset\n",
    "# output_file = 'resume_job_training_data.csv'\n",
    "# labeled_training_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9bFxLn8Hzky"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plt.hist(training_pairs[\"final_score\"], bins=30, color='skyblue', edgecolor='black')\n",
    "# plt.title(\"Distribution of Resume–Job Fit Scores\")\n",
    "# plt.xlabel(\"Final Composite Score\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_n6zBnVK-aX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
